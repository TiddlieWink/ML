{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Random Forest TFIDF",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "AMmlrMKe7mfH"
      },
      "source": [
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "\r\n",
        "#pre processing \r\n",
        "import re\r\n",
        "import string\r\n",
        "import nltk.downloader\r\n",
        "from nltk.corpus import stopwords\r\n",
        "from nltk.stem import WordNetLemmatizer\r\n",
        "from nltk.stem.porter import PorterStemmer\r\n",
        "\r\n",
        "#tree\r\n",
        "import random\r\n",
        "\r\n",
        "#serialization\r\n",
        "import dill"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6RRy8hEw9fMt"
      },
      "source": [
        "# Define Text Cleaner\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OuV5L5Zn9nRZ"
      },
      "source": [
        "class TextPreprocessor():\r\n",
        "    \r\n",
        "    '''\r\n",
        "    Preprocess a string for NLP\r\n",
        "    \r\n",
        "    Inputs\r\n",
        "        mode: 'int' 1 for lematization, 0 for stemming\r\n",
        "    '''\r\n",
        "\r\n",
        "    def __init__(self, mode: 'int' = 1):\r\n",
        "\r\n",
        "        self.mode = mode\r\n",
        "\r\n",
        "        nltk.download('stopwords')\r\n",
        "        self.stop_words = stopwords.words('english')\r\n",
        "\r\n",
        "        if mode:\r\n",
        "            \r\n",
        "            nltk.download('wordnet')\r\n",
        "            nltk.download('averaged_perceptron_tagger')\r\n",
        "            self.tokenizer = WordNetLemmatizer()\r\n",
        "            \r\n",
        "        else:\r\n",
        "            \r\n",
        "            self.tokenizer = PorterStemmer()\r\n",
        "            \r\n",
        "    #run the fulling cleaning process\r\n",
        "    def prepText(self,text):\r\n",
        "    \r\n",
        "        if self.mode:\r\n",
        "            \r\n",
        "            text = self._cleanText(text)\r\n",
        "            words = self._tokenize(text)\r\n",
        "            words = self._removeStops(words)\r\n",
        "            words = self._lematize(words)\r\n",
        "            \r\n",
        "        else:\r\n",
        "            \r\n",
        "            text = self._cleanText(text)\r\n",
        "            words = self._tokenize(text)\r\n",
        "            words = self._removeStops(words)\r\n",
        "            words = self._stem(words)\r\n",
        "    \r\n",
        "        return words\r\n",
        "            \r\n",
        "    def _cleanText(self,text):\r\n",
        "\r\n",
        "        #cast to lower case\r\n",
        "        text = text.lower()\r\n",
        "    \r\n",
        "        #define regex for links and remove them\r\n",
        "        text_dn =re.sub(r'http\\S+','',text,)\r\n",
        "    \r\n",
        "        #remove non letters ,digits, and spaces\r\n",
        "        text_ascii = \"\".join([char for char in text_dn if char in string.ascii_lowercase+' '+string.digits])\r\n",
        "\r\n",
        "        return text_ascii\r\n",
        "    \r\n",
        "    def _tokenize(self,text):\r\n",
        "    \r\n",
        "        #splits words on spaces\r\n",
        "        words = text.split(\" \")\r\n",
        "\r\n",
        "        #drop words that are just spaces\r\n",
        "        words = [word for word in words if word != \"\"]\r\n",
        "\r\n",
        "        return words\r\n",
        "    \r\n",
        "    def _removeStops(self,words):\r\n",
        "    \r\n",
        "        #drop all stops words\r\n",
        "        return [word for word in words if  word not in self.stop_words]\r\n",
        "    \r\n",
        "    def _lematize(self,words):\r\n",
        "    \r\n",
        "        #lematize cleaned words\r\n",
        "        return [self.tokenizer.lemmatize(word) for word in words]\r\n",
        "  \r\n",
        "    def _stem(self, words):\r\n",
        "        \r\n",
        "        #stem cleaned words\r\n",
        "        return [self.tokenizer.stem(word) for word in words]\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tsoxacRS9qTW"
      },
      "source": [
        "# Define Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xp_pkDxF9p09"
      },
      "source": [
        "class RandomForest():\r\n",
        "    \r\n",
        "    def __init__(self,X_train, y_train, n_estimators, max_features, max_depth, min_samples_split):\r\n",
        "        tree_ls = list()\r\n",
        "        oob_ls = list()\r\n",
        "        for i in range(n_estimators):\r\n",
        "            X_bootstrap, y_bootstrap, X_oob, y_oob = self.draw_bootstrap(X_train, y_train)\r\n",
        "            tree = self.build_tree(X_bootstrap, y_bootstrap, max_features, max_depth, min_samples_split)\r\n",
        "            tree_ls.append(tree)\r\n",
        "            oob_error = self.oob_score(tree, X_oob, y_oob)\r\n",
        "            oob_ls.append(oob_error)\r\n",
        "        print(\"OOB estimate: {:.2f}\".format(np.mean(oob_ls)))\r\n",
        "        self.tree_ls = tree_ls  \r\n",
        "\r\n",
        "        \r\n",
        "    def entropy(self, p):\r\n",
        "        \r\n",
        "        if p == 0:\r\n",
        "            \r\n",
        "            return 0\r\n",
        "        \r\n",
        "        elif p == 1:\r\n",
        "            \r\n",
        "            return 0\r\n",
        "        \r\n",
        "        else:\r\n",
        "            return - (p * np.log2(p) + (1 - p) * np.log2(1-p))\r\n",
        "        \r\n",
        "    def information_gain(self, left_child, right_child):\r\n",
        "        \r\n",
        "        parent = left_child + right_child\r\n",
        "        p_parent = parent.count(1) / len(parent) if len(parent) > 0 else 0\r\n",
        "        p_left = left_child.count(1) / len(left_child) if len(left_child) > 0 else 0\r\n",
        "        p_right = right_child.count(1) / len(right_child) if len(right_child) > 0 else 0\r\n",
        "        info_gain_p = self.entropy(p_parent)\r\n",
        "        info_gain_l = self.entropy(p_left)\r\n",
        "        info_gain_r = self.entropy(p_right)\r\n",
        "        return (info_gain_p \r\n",
        "                - len(left_child) / len(parent) * info_gain_l \r\n",
        "                - len(right_child) / len(parent) * info_gain_r)\r\n",
        "    \r\n",
        "    \r\n",
        "\r\n",
        "    def draw_bootstrap(self, X_train, y_train):\r\n",
        "        \r\n",
        "        bootstrap_indices = list(np.random.choice(range(len(X_train)), len(X_train), replace = True))\r\n",
        "        oob_indices = [i for i in range(len(X_train)) if i not in bootstrap_indices]\r\n",
        "        self.X_bootstrap = X_train.iloc[bootstrap_indices].values\r\n",
        "        self.y_bootstrap = y_train[bootstrap_indices]\r\n",
        "        self.X_oob = X_train.iloc[oob_indices].values\r\n",
        "        self.y_oob = y_train[oob_indices]\r\n",
        "        return self.X_bootstrap, self.y_bootstrap, self.X_oob, self.y_oob\r\n",
        "    \r\n",
        "    def oob_score(self,tree, X_test, y_test):\r\n",
        "        \r\n",
        "        mis_label = 0\r\n",
        "        for i in range(len(X_test)):\r\n",
        "            pred = self.predict_tree(tree, X_test[i])\r\n",
        "            if pred != y_test[i]:\r\n",
        "                mis_label += 1\r\n",
        "        return mis_label / len(X_test)\r\n",
        "    \r\n",
        "    def find_split_point(self, X_bootstrap, y_bootstrap, max_features):\r\n",
        "        feature_ls = list()\r\n",
        "        num_features = len(X_bootstrap[0])\r\n",
        "        \r\n",
        "        while len(feature_ls) <= max_features:\r\n",
        "            feature_idx = random.sample(range(num_features), 1)\r\n",
        "            if feature_idx not in feature_ls:\r\n",
        "                feature_ls.extend(feature_idx)\r\n",
        "        \r\n",
        "        best_info_gain = -999\r\n",
        "        node = None\r\n",
        "        for feature_idx in feature_ls:\r\n",
        "            for split_point in X_bootstrap[:,feature_idx]:\r\n",
        "                left_child = {'X_bootstrap': [], 'y_bootstrap': []}\r\n",
        "                right_child = {'X_bootstrap': [], 'y_bootstrap': []}\r\n",
        "                \r\n",
        "                # split children for continuous variables\r\n",
        "                if type(split_point) in [int, float]:\r\n",
        "                    for i, value in enumerate(X_bootstrap[:,feature_idx]):\r\n",
        "                        if value <= split_point:\r\n",
        "                            left_child['X_bootstrap'].append(X_bootstrap[i])\r\n",
        "                            left_child['y_bootstrap'].append(y_bootstrap[i])\r\n",
        "                        else:\r\n",
        "                            right_child['X_bootstrap'].append(X_bootstrap[i])\r\n",
        "                            right_child['y_bootstrap'].append(y_bootstrap[i])\r\n",
        "                # split children for categoric variables\r\n",
        "                else:\r\n",
        "                    for i, value in enumerate(X_bootstrap[:,feature_idx]):\r\n",
        "                        if value == split_point:\r\n",
        "                            left_child['X_bootstrap'].append(X_bootstrap[i])\r\n",
        "                            left_child['y_bootstrap'].append(y_bootstrap[i])\r\n",
        "                        else:\r\n",
        "                            right_child['X_bootstrap'].append(X_bootstrap[i])\r\n",
        "                            right_child['y_bootstrap'].append(y_bootstrap[i])\r\n",
        "                \r\n",
        "                split_info_gain = self.information_gain(left_child['y_bootstrap'], right_child['y_bootstrap'])\r\n",
        "                if split_info_gain > best_info_gain:\r\n",
        "                    best_info_gain = split_info_gain\r\n",
        "                    left_child['X_bootstrap'] = np.array(left_child['X_bootstrap'])\r\n",
        "                    right_child['X_bootstrap'] = np.array(right_child['X_bootstrap'])\r\n",
        "                    node = {'information_gain': split_info_gain, \r\n",
        "                            'left_child': left_child, \r\n",
        "                            'right_child': right_child, \r\n",
        "                            'split_point': split_point,\r\n",
        "                            'feature_idx': feature_idx}\r\n",
        "                    \r\n",
        "        return node\r\n",
        "    \r\n",
        "    def terminal_node(self, node):\r\n",
        "        \r\n",
        "        y_bootstrap = node['y_bootstrap']\r\n",
        "        pred = max(y_bootstrap, key = y_bootstrap.count)\r\n",
        "        return pred\r\n",
        "    \r\n",
        "    def split_node(self, node, max_features, min_samples_split, max_depth, depth):\r\n",
        "        left_child = node['left_child']\r\n",
        "        right_child = node['right_child']    \r\n",
        "    \r\n",
        "        del(node['left_child'])\r\n",
        "        del(node['right_child'])\r\n",
        "        \r\n",
        "        if len(left_child['y_bootstrap']) == 0 or len(right_child['y_bootstrap']) == 0:\r\n",
        "            empty_child = {'y_bootstrap': left_child['y_bootstrap'] + right_child['y_bootstrap']}\r\n",
        "            node['left_split'] = self.terminal_node(empty_child)\r\n",
        "            node['right_split'] = self.terminal_node(empty_child)\r\n",
        "            return\r\n",
        "        \r\n",
        "        if depth >= max_depth:\r\n",
        "            node['left_split'] = self.terminal_node(left_child)\r\n",
        "            node['right_split'] = self.terminal_node(right_child)\r\n",
        "            return node\r\n",
        "        \r\n",
        "        if len(left_child['X_bootstrap']) <= min_samples_split:\r\n",
        "            node['left_split'] = node['right_split'] = self.terminal_node(left_child)\r\n",
        "        else:\r\n",
        "            node['left_split'] = self.find_split_point(left_child['X_bootstrap'], left_child['y_bootstrap'], max_features)\r\n",
        "            self.split_node(node['left_split'], max_depth, min_samples_split, max_depth, depth + 1)\r\n",
        "        if len(right_child['X_bootstrap']) <= min_samples_split:\r\n",
        "            node['right_split'] = node['left_split'] = self.terminal_node(right_child)\r\n",
        "        else:\r\n",
        "            node['right_split'] = self.find_split_point(right_child['X_bootstrap'], right_child['y_bootstrap'], max_features)\r\n",
        "            self.split_node(node['right_split'], max_features, min_samples_split, max_depth, depth + 1)\r\n",
        "        \r\n",
        "    def build_tree(self,X_bootstrap, y_bootstrap, max_depth, min_samples_split, max_features):\r\n",
        "        \r\n",
        "        root_node = self.find_split_point(X_bootstrap, y_bootstrap, max_features)\r\n",
        "        self.split_node(root_node, max_features, min_samples_split, max_depth, 1)\r\n",
        "        return root_node\r\n",
        " \r\n",
        "    \r\n",
        "\r\n",
        "    def predict_tree(self, tree, X_test):\r\n",
        "        feature_idx = tree['feature_idx']\r\n",
        "        \r\n",
        "        if X_test[feature_idx] <= tree['split_point']:\r\n",
        "            if type(tree['left_split']) == dict:\r\n",
        "                return self.predict_tree(tree['left_split'], X_test)\r\n",
        "            else:\r\n",
        "                value = tree['left_split']\r\n",
        "                return value\r\n",
        "        else:\r\n",
        "            if type(tree['right_split']) == dict:\r\n",
        "                return self.predict_tree(tree['right_split'], X_test)\r\n",
        "            else:\r\n",
        "                return tree['right_split']\r\n",
        "            \r\n",
        "    \r\n",
        "\r\n",
        "    def predict_rf(self, X_test):\r\n",
        "        pred_ls = list()\r\n",
        "        for i in range(len(X_test)):\r\n",
        "            ensemble_preds = [self.predict_tree(tree, X_test.values[i]) for tree in self.tree_ls]\r\n",
        "            final_pred = max(ensemble_preds, key = ensemble_preds.count)\r\n",
        "            pred_ls.append(final_pred)\r\n",
        "        return np.array(pred_ls)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o476gRR99_Ub"
      },
      "source": [
        "# Define TFIDF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TtsPkzAv-BF2"
      },
      "source": [
        "class TFIDF():\r\n",
        "    \r\n",
        "    def __init__(self, data = None, column1 = 0, column2 = 0):\r\n",
        "        \r\n",
        "        self.data = data\r\n",
        "        self.column1 = column1\r\n",
        "        self.column2 = column2\r\n",
        "        self.N = len(self.data[self.column1])\r\n",
        "        \r\n",
        "    \r\n",
        "    #runs the embedding\r\n",
        "    def get_tfidf(self):\r\n",
        "        \r\n",
        "        self._word_frequncey()\r\n",
        "        self._get_unique_words()\r\n",
        "        self._document_frequencey()\r\n",
        "        self._compute_tfidf()\r\n",
        "        \r\n",
        "        return self.score_matrix\r\n",
        "        \r\n",
        "    #calculates the word freqeuncey per document   \r\n",
        "    def _word_frequncey(self):\r\n",
        "        \r\n",
        "        self.texts = {}\r\n",
        "        \r\n",
        "        for word_list ,text in zip(self.data[self.column1],self.data[self.column2]):\r\n",
        "            \r\n",
        "            word_count = {}\r\n",
        "            \r\n",
        "            for word in word_list:\r\n",
        "                \r\n",
        "                if word in word_count:\r\n",
        "                    \r\n",
        "                    word_count[word] = word_count[word] + 1\r\n",
        "                \r\n",
        "                else:\r\n",
        "                    \r\n",
        "                    word_count[word] = 1\r\n",
        "                    \r\n",
        "            self.texts[text] = word_count\r\n",
        "            \r\n",
        "        return self.texts\r\n",
        "    \r\n",
        "    #gets a list of unique words in the DataFrame\r\n",
        "    def _get_unique_words(self):\r\n",
        "        \r\n",
        "        \r\n",
        "        self.unique_words  = []\r\n",
        "        \r\n",
        "        for text in self.texts.values(): \r\n",
        "        \r\n",
        "            for key in text.keys():\r\n",
        "                \r\n",
        "                self.unique_words.append(key)\r\n",
        "                \r\n",
        "        self.unique_words = set(self.unique_words)\r\n",
        "        \r\n",
        "        self.M = len(self.unique_words)\r\n",
        "        \r\n",
        "        return self.unique_words\r\n",
        "    \r\n",
        "    #calculates the codument frequency \r\n",
        "    def _document_frequencey(self):\r\n",
        "        \r\n",
        "        self.document_frequencey = {}\r\n",
        "        \r\n",
        "        for word1 in self.unique_words:\r\n",
        "            \r\n",
        "            count = 0\r\n",
        "        \r\n",
        "            for doc in self.data[self.column1]:\r\n",
        "            \r\n",
        "                \r\n",
        "                for word2  in set(doc):\r\n",
        "                    \r\n",
        "                    if word2 == word1:\r\n",
        "                        \r\n",
        "                        count = count + 1/self.N\r\n",
        "            \r\n",
        "            self.document_frequencey[word1] = count\r\n",
        "            \r\n",
        "        return self.document_frequencey\r\n",
        "    \r\n",
        "    #computes the embedding matrix\r\n",
        "    def _compute_tfidf(self):\r\n",
        "        \r\n",
        "        self.score_matrix = np.zeros((self.N,self.M))\r\n",
        "        \r\n",
        "        for i, text in enumerate(self.data[self.column2]):\r\n",
        "        \r\n",
        "            for j, word in enumerate(self.unique_words):\r\n",
        "                \r\n",
        "                if word in self.texts[text]:\r\n",
        "            \r\n",
        "                    self.score_matrix[i,j] = self.texts[text][word] * np.log(1.0/self.document_frequencey[word])\r\n",
        "                \r\n",
        "        return self.score_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36vGAfrn-dkT"
      },
      "source": [
        "# Import Data and Pre Process"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SU_fVu7Q9pl4"
      },
      "source": [
        "df = pd.read_csv(\"Tweet Data.csv\")\r\n",
        "\r\n",
        "processor = TextPreprocessor(mode=1)\r\n",
        "\r\n",
        "df = df.reindex(columns = ['message','hashtag','label', 'words'])\r\n",
        "\r\n",
        "df['words'] = df['words'].astype('object')\r\n",
        "\r\n",
        "for i,text in enumerate(df['message']):\r\n",
        "\r\n",
        "  df.at[i, 'words'] = processor.prepText(text)\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uckUPai-tLI"
      },
      "source": [
        "# Embedd with TFIDF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uoijZ_b79ox0"
      },
      "source": [
        "embedder = TFIDF(data = df, column1 = 'words', column2 = 'message')\r\n",
        "\r\n",
        "score_matrix = embedder.get_tfidf()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LT9PureR-6ZY"
      },
      "source": [
        "# Train Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3NooYSwp-yU0"
      },
      "source": [
        "X_train = pd.DataFrame(score_matrix[:-3000])\r\n",
        "y_train = df['label'].to_numpy()[:-3000]\r\n",
        "\r\n",
        "X_val = pd.DataFrame(score_matrix[-3000:-1500])\r\n",
        "y_val = df['label'].to_numpy()[-3000:-1500]\r\n",
        "\r\n",
        "X_test = pd.DataFrame(score_matrix[-1500:])\r\n",
        "y_test = df['label'].to_numpy()[-1500:]\r\n",
        "\r\n",
        "\r\n",
        "forest = RandomForest(X_train, y_train, n_estimators= 10, max_features = np.round(score_matrix.shape[1]**0.5), max_depth = 3, min_samples_split = 2)\r\n",
        "\r\n",
        "preds = forest.predict_rf(X_val)\r\n",
        "\r\n",
        "acc = sum(preds == y_val) / len(y_val)\r\n",
        "print(\"Testing accuracy: {}\".format(np.round(acc,3)))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}